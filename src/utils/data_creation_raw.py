93% di spazio di archiviazione utilizzato … 
Se esaurisci lo spazio, non puoi creare, modificare e caricare file. Acquista 100 GB di spazio di archiviazione a 1,99 € 0,49 € per 1 mese (prezzo personalizzato).

# -*- coding: utf-8 -*-
"""Datasets_list.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Afo5p7nTFZbLrbJJSWJ2TvnB391L3uWT
"""

!install pandas
!pip install datasets

from google.colab import drive
drive.mount('/content/drive')

"""#PICTOGRAMS"""

from zipfile import ZipFile
with ZipFile("/content/drive/MyDrive/PICTOBERT/data/Pittogrammi.zip") as zipObj:
  zipObj.extractall("/content/pictoimg")

"""#ARASAAC"""

import pandas as pd
pic_map = pd.read_csv("/content/drive/MyDrive/PICTOBERT/data/arasaac_mapping.csv")
print(pic_map.size)
pic_map.head()

"""# Dataset preprocessing"""

import pandas as pd

def write_dataframe_to_text_file(df, file_name, col):
    # Full path of the text file in the current directory
    file_path = f'./{file_name}'

    # Write each row of the dataframe to the text file
    with open(file_path, 'w', encoding='utf-8') as file:
        for index, row in df.iterrows():
            line = row[col]
            # Check if the line is a non-empty string
            if isinstance(line, str) and line.strip():
                file.write(str(line) + '\n')

    # Print a message indicating that the file is created
    print(f"File '{file_name}' is created")

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('wordnet')

def preprocess(sentence):
    #convert to lowercase
    sentence = sentence.lower()
    # Remove URLs
    sentence = re.sub(r"http\S+|www\S+|https\S+", "", sentence, flags=re.MULTILINE)

    # Remove mentions
    sentence = re.sub(r"@\w+", "", sentence)

    # Replace genitive ('s) with " of "
    sentence = re.sub(r"'s\b", " of ", sentence)

    # Remove parentheses and their contents
    sentence = re.sub(r"\([^()]*\)", "", sentence)

    # Remove dashes and replace with space
    sentence = sentence.replace("-", " ")

    # Replace "no". with "number"
    sentence = sentence.replace("no.", "number")

    # Remove unrecognized characters
    sentence = re.sub(r"[^\w\s\d.,;!?]", "", sentence)

    # Remove "⁇" symbol
    sentence = sentence.replace("⁇", "")

    # Tokenize sentence
    tokens = nltk.word_tokenize(sentence)

    # Initialize lemmatizer
    lemmatizer = WordNetLemmatizer()

    # Lemmatize verbs and keep punctuation
    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') if token.isalnum() else token for token in tokens]

    # Join tokens back into a sentence
    preprocessed_sentence = ' '.join(lemmatized_tokens)

    return preprocessed_sentence

"""#CONCEPTUAL CAPTIONS DS

image raw descriptions are harvested from the web and represent a wider variety of styles. Raw descriptions are harvested from the Alt-text HTML attribute associated with web images. To have the current version of the captions, an automatic pipeline extracted, filtered and transformed candidate image/caption pairs, with the goal of achieving a balance of cleanliness, informativeness, fluency, and learnability of the resulting captions.
We take the caption column.

https://huggingface.co/datasets/conceptual_captions


This code performs several tasks related to the Conceptual Captions dataset:

It loads the dataset using the Hugging Face datasets library.

Preprocesses the caption text by removing special characters, tokenizes the sentences, and stores the preprocessed sentences in a DataFrame.

Downloads additional data by matching tokens in the sentences to a mapping file (pic_map) that associates words with image IDs.

Extracts information about image tokens and token matches and stores it in a DataFrame.

Drops duplicate sentences and sentences with no associated image tokens.

Analyzes and visualizes the dataset, including sentence lengths, token counts per sentence, and the distribution of the number of image tokens per sentence.
"""

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("conceptual_captions")

import pandas as pd
df = pd.DataFrame(dataset['train'])
df = df[['caption']]

df.head()

df["preprocessed_sentence"] = df["caption"].apply(preprocess)

df= df.head(10000) #10.000
write_dataframe_to_text_file( df, 'output_CC.txt', "preprocessed_sentence")

df.head(20)

sentence_word = open("/content/output_CC.txt",'r').readlines()
sw = [s.rstrip() for s in sentence_word ]
len(sw)
df= pd.DataFrame(data=sw, columns=["sentence"])
df.size

df["imgs"] = ""
#df["index"] = ""#index of the word sense into the dataframe pic_map
df["token_found"] = ""
df.head()

!pip install datasets transformers torch pytorch_lightning

from transformers import ViltConfig
from transformers import ViltProcessor, ViltForMaskedLM
import torch
processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
tokenizer = processor.tokenizer

import os
import random
import numpy as np
import torch
import pandas as pd
from urllib.request import urlopen
from zipfile import ZipFile
from io import BytesIO

from tqdm import tqdm

def download_data(file_path, picto_file_path, index_file_path):
    f = open(file_path, 'r')
    f2 = open(picto_file_path, 'w')
    f3 = open(index_file_path, 'w')
    new_sentences = []
    count = 0
    sentences = []
    word_found_list = []
    imgs_list = []
    picto_list_str = []
    new_df = pd.DataFrame(columns=["sentence", "token_found", "imgs", "picto"])

    for l in tqdm(f.readlines()):
        picto_list = []
        word_found = []  # Array to store matching tokens in each row
        processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
        tokenizer = processor.tokenizer
        tokenized_sentence = tokenizer.tokenize(l.rstrip())

        # Check if at least one token is present in pic_map["word"] and has an associated image
        for token in tokenized_sentence:
            if (pic_map["word"].eq(token)).any():
                pictoid = pic_map.loc[pic_map["word"] == token, "pictogram_id"].tolist()[0]
                if os.path.exists("/content/pictoimg/Pittogrammi/" + str(int(pictoid)) + ".png"):
                    index = pic_map.loc[pic_map["word"] == token].index.tolist()

                    picto_list.append(int(pictoid))
                    word_found.append(token)

        # If no tokens satisfy the conditions, skip the current sentence
        # If picto_list is empty, it means that no token has satisfied the required conditions.
        if not picto_list:
            continue

        sentence = l.rstrip()
        sentences.append(sentence)
        word_found_list.append(' - '.join(word_found))
        imgs_list.append(picto_list)
        picto_list_str.append(' '.join(map(str, picto_list)))

    new_df["sentence"] = sentences
    new_df["token_found"] = word_found_list
    new_df["imgs"] = imgs_list
    new_df["picto"] = picto_list_str

    return new_df

df_total=download_data("/content/output_CC.txt", "/content/list_picto_CC.txt", "/content/idx_picto_CC.txt")

df_total.head(20)

df_total.drop_duplicates(subset=['sentence'], keep='first', inplace=True)
df_total = df_total[df_total['imgs'].apply(lambda x: len(x) > 0)]

df_total.head(20)

df_total.head(65956)

write_dataframe_to_text_file(df_total, 'CC_preprocess.txt', "sentence")
write_dataframe_to_text_file(df_total, 'CC_picto_preprc.txt', "picto")

"""##Download dataset"""

sentence_word = open("/content/drive/MyDrive/VILT/Dataset_testo/CC_preprocess.txt",'r').readlines()
sw = [s.rstrip() for s in sentence_word ]
len(sw)
df= pd.DataFrame(data=sw, columns=["sentence"])
df.size
#df["imgs"] = ""
#df["index"] = ""#index of the word sense into the dataframe pic_map
#df["token_found"] = ""
df.head()
piclist = open("/content/drive/MyDrive/VILT/Dataset_testo/CC_picto_preprc.txt",'r').readlines()
pic = [s.rstrip() for s in piclist]
len(pic)
import pandas as pd
df_pic = pd.DataFrame(data=pic, columns=["picto"])
df_pic.head()

df_total=pd.concat([df, df_pic], axis=1)
df_total.head(100)

empty_rows = df_total[df_total["picto"].isnull()]
empty_rows_count = df_total["picto"].isnull().sum()
empty_rows_count

"""###STUDY of dataset"""

# Calcola la lunghezza di ciascuna frase nella colonna 'sentence'
sentence_lengths = df_total['sentence'].apply(lambda x: len(x.split()))

# Calcola la lunghezza media delle frasi
max = sentence_lengths.max()

print("La frase di lunghezza massima è:")
print("Lunghezza:", max)
print(sentence_lengths)

import matplotlib.pyplot as plt

# Calcola la lunghezza di ciascuna frase nella colonna 'sentence'
sentence_lengths = df_total['sentence'].apply(lambda x: len(x.split()))

# Calcola la lunghezza media delle frasi
average_length = sentence_lengths.mean()
print("mean of length of sentence:", average_length )
# Crea un array di indici per le frasi
indices = range(len(sentence_lengths))

# Crea un grafico a barre per visualizzare le lunghezze delle frasi
plt.figure(figsize=(10, 6))
plt.bar(indices, sentence_lengths)
plt.axhline(y=average_length, color='red', linestyle='--', label='Average Length')
plt.xlabel('Sentence')
plt.ylabel('Length')
plt.title('Sentence Lengths')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Calcola il numero di token per ogni sentence nella colonna "picto"
df_total['token_count'] = df_total['picto'].apply(lambda x: len(x.split(' ')))

# Calcola il valore medio di token per sentence
average_token_count = np.mean(df_total['token_count'])
print("Mean number of tokens found into a sentence:", average_token_count)
# Crea un array di indici per le sentence
indices = np.arange(len(df_total))

# Imposta la larghezza delle barre
bar_width = 0.8

# Crea il grafico a barre
plt.figure(figsize=(10, 6))
plt.bar(indices, df_total['token_count'], width=bar_width)
plt.axhline(y=average_token_count, color='red', linestyle='--', label='Average Token Count')

# Personalizza l'asse x
plt.xlabel('Sentence')
plt.ylabel('Token Count')
plt.title('Token Count per Sentence')
plt.xticks(indices)

# Mostra la legenda
plt.legend()

# Visualizza il grafico
plt.show()

# Calcola il numero di picto per ogni sentence nella colonna "picto"
df_total['picto_count'] = df_total['picto'].apply(lambda x: len(x.split(' ')))

# Calcola il numero di frasi per ogni numero di picto
picto_counts = df_total['picto_count'].value_counts().sort_index()

# Crea un array di indici per il numero di picto
indices = np.arange(len(picto_counts))

# Imposta la larghezza delle barre
bar_width = 0.8

# Crea il grafico a barre
plt.figure(figsize=(10, 6))
plt.bar(indices, picto_counts, width=bar_width)
#plt.axhline(y=picto_counts.mean(), color='red', linestyle='--', label='Average Sentence Count')

# Personalizza l'asse x
plt.xlabel('Number of Picto per Sentence')
plt.ylabel('Sentence Count')
plt.title('Distribution of Picto per Sentence')
plt.xticks(indices, picto_counts.index)

# Mostra la legenda
#plt.legend()

# Visualizza il grafico
plt.show()

"""#SBU Captions
vilt dataset for training.
SBU Captioned Photo Dataset is a collection of associated captions and images from Flickr.All the data is contained in training split. The training set has 1M instances.
we take the caption col.

https://huggingface.co/datasets/sbu_captions

This code processes and analyzes the "SBU Captions" dataset, including text preprocessing, data exploration, and saving preprocessed data to text files.

"""

from datasets import load_dataset

dataset = load_dataset("sbu_captions")

import pandas as pd
df = pd.DataFrame(dataset['train'])
df.head()

import pandas as pd
df = pd.DataFrame(dataset['train'])

df.head(20)

import pandas as pd

def write_dataframe_to_text_file(df, file_name, col):
    # Percorso completo del file di testo nella directory corrente
    file_path = f'./{file_name}'

    # Scrivi ogni riga del dataframe nel file di testo
    with open(file_path, 'w', encoding='utf-8') as file:
        for index, row in df.iterrows():
            line = row[col]
            #if isinstance(line, str) and line.strip():  # Verifica se la riga è una stringa non vuota
            file.write(str(line) + '\n')

    print(f"File '{file_name}' is created")

!pip install datasets transformers torch pytorch_lightning

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('wordnet')

def preprocess(sentence):
    #convert to lowercase
    sentence = sentence.lower()
    # Remove URLs
    sentence = re.sub(r"http\S+|www\S+|https\S+", "", sentence, flags=re.MULTILINE)

    # Remove mentions
    sentence = re.sub(r"@\w+", "", sentence)

    # Replace genitive ('s) with " of "
    sentence = re.sub(r"'s\b", " of ", sentence)

    # Remove parentheses and their contents
    sentence = re.sub(r"\([^()]*\)", "", sentence)

    # Remove dashes and replace with space
    sentence = sentence.replace("-", " ")

    # Replace "no". with "number"
    sentence = sentence.replace("no.", "number")

    # Remove unrecognized characters
    sentence = re.sub(r"[^\w\s\d.,;!?]", "", sentence)

    # Remove "⁇" symbol
    sentence = sentence.replace("⁇", "")

    # Tokenize sentence
    tokens = nltk.word_tokenize(sentence)

    # Initialize lemmatizer
    lemmatizer = WordNetLemmatizer()

    # Lemmatize verbs and keep punctuation
    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') if token.isalnum() else token for token in tokens]

    # Join tokens back into a sentence
    preprocessed_sentence = ' '.join(lemmatized_tokens)

    return preprocessed_sentence

def preprocess_punct(sentence):
    #convert to lowercase
    sentence = sentence.lower()
    # Remove URLs
    sentence = re.sub(r"http\S+|www\S+|https\S+", "", sentence, flags=re.MULTILINE)

    # Remove mentions
    sentence = re.sub(r"@\w+", "", sentence)

    # Replace genitive ('s) with " of "
    sentence = re.sub(r"'s\b", " of ", sentence)

    # Remove parentheses and their contents
    sentence = re.sub(r"\([^()]*\)", "", sentence)

    # Remove dashes and replace with space
    sentence = sentence.replace("-", " ")

    # Replace "no". with "number"
    sentence = sentence.replace("no.", "number")

    # Remove unrecognized characters
    sentence = re.sub(r"[^\w\s\d.,;!?]", "", sentence)

    # Remove punctuation
    sentence = re.sub(r"[.,;:?!]", "", sentence)

    # Remove multiple suspension points
    sentence = re.sub(r"\.{2,}", "", sentence)

    # Remove "⁇" symbol
    sentence = sentence.replace("⁇", "")

    # Tokenize sentence
    tokens = nltk.word_tokenize(sentence)

    # Initialize lemmatizer
    lemmatizer = WordNetLemmatizer()

    # Lemmatize verbs and keep punctuation
    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') if token.isalnum() else token for token in tokens]

    # Join tokens back into a sentence
    preprocessed_sentence = ' '.join(lemmatized_tokens)

    return preprocessed_sentence

from transformers import ViltConfig
from transformers import ViltProcessor, ViltForMaskedLM
import torch
processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
tokenizer = processor.tokenizer

processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
tokenizer = processor.tokenizer

df["preprocessed_sentence"] = df["caption"].apply(preprocess)

df["preprocessed_sentence"] = df["caption"].apply(preprocess_punct)

df.head()

df= df.head(20000) #10.000
write_dataframe_to_text_file( df, 'output_SBU.txt', "preprocessed_sentence")

df.head()

sentence_word = open("/content/output_SBU.txt",'r').readlines()
sw = [s.rstrip() for s in sentence_word ]
len(sw)
df= pd.DataFrame(data=sw, columns=["sentence"])
df.size

#df["imgs"] = ""
#df["index"] = ""#index of the word sense into the dataframe pic_map
#df["token_found"] = ""
#df.head()

df.head()

df.size

sentence_to_find = "green turtle catch by the daylight, they usually be not see after 4am more or less"
result = df[df['sentence'] == sentence_to_find]
result

import os
import random
import numpy as np
import torch
import pandas as pd
from urllib.request import urlopen
from zipfile import ZipFile
from io import BytesIO

from tqdm import tqdm

def download_data(file_path, picto_file_path, index_file_path):
    f = open(file_path, 'r')
    f2 = open(picto_file_path, 'w')
    f3 = open(index_file_path, 'w')
    new_sentences = []
    count = 0
    sentences = []
    word_found_list = []
    imgs_list = []
    picto_list_str = []
    new_df = pd.DataFrame(columns=["sentence", "token_found", "imgs", "picto"])

    for l in tqdm(f.readlines()):
        picto_list = []
        word_found = []  # Array to store matching tokens in each row
        processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
        tokenizer = processor.tokenizer
        #print("sentence" , l.rstrip())
        tokenized_sentence = tokenizer.tokenize(l.rstrip())
        #print("tokenized_sentence" , tokenized_sentence)

        # Check if at least one token is present in pic_map["word"] and has an associated image
        for token in tokenized_sentence:
            if (pic_map["word"].eq(token)).any():
                pictoid = pic_map.loc[pic_map["word"] == token, "pictogram_id"].tolist()[0]
                if os.path.exists("/content/pictoimg/Pittogrammi/" + str(int(pictoid)) + ".png"):
                    index = pic_map.loc[pic_map["word"] == token].index.tolist()

                    picto_list.append(int(pictoid))
                    word_found.append(token)

        # If no tokens satisfy the conditions, skip the current sentence
        # If picto_list is empty, it means that no token has satisfied the required conditions.
        if not picto_list:
            continue

        sentence = l.rstrip()
        sentences.append(sentence)
        word_found_list.append(' - '.join(word_found))
        imgs_list.append(picto_list)
        picto_list_str.append(' '.join(map(str, picto_list)))
        """
        print("token", word_found_list)
        print("picto_list", picto_list)
        """

    new_df["sentence"] = sentences
    new_df["token_found"] = word_found_list
    new_df["imgs"] = imgs_list
    new_df["picto"] = picto_list_str

    return new_df

import os
import random
import numpy as np
import torch
import pandas as pd
from urllib.request import urlopen
from zipfile import ZipFile
from io import BytesIO

from tqdm import tqdm

def download_data_cut(file_path, picto_file_path, index_file_path):
    f = open(file_path, 'r')
    f2 = open(picto_file_path, 'w')
    f3 = open(index_file_path, 'w')
    new_sentences = []
    count = 0
    sentences = []
    word_found_list = []
    imgs_list = []
    picto_list_str = []
    new_df = pd.DataFrame(columns=["sentence", "token_found", "imgs", "picto"])

    for l in tqdm(f.readlines()):
        picto_list = []
        word_found = []  # Array to store matching tokens in each row
        processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
        tokenizer = processor.tokenizer
        #tokenized_sentence = tokenizer.tokenize(l.rstrip())
        tokenized_sentence = tokenizer.tokenize(l.rstrip(), truncation=True, max_length=40)


        # Check if the tokenized sentence exceeds the maximum length of 40 tokens
        if len(tokenized_sentence) > 40:
            continue

        # Check if at least one token is present in pic_map["word"] and has an associated image
        for token in tokenized_sentence:
            if (pic_map["word"].eq(token)).any():
                pictoid = pic_map.loc[pic_map["word"] == token, "pictogram_id"].tolist()[0]
                if os.path.exists("/content/pictoimg/Pittogrammi/" + str(int(pictoid)) + ".png"):
                    index = pic_map.loc[pic_map["word"] == token].index.tolist()
                    picto_list.append(int(pictoid))
                    word_found.append(token)

        # If no tokens satisfy the conditions, skip the current sentence
        # If picto_list is empty, it means that no token has satisfied the required conditions.
        if not picto_list:
            continue

        sentence = l.rstrip()
        sentences.append(sentence)
        word_found_list.append(' - '.join(word_found))
        imgs_list.append(picto_list)
        picto_list_str.append(' '.join(map(str, picto_list)))

    new_df["sentence"] = sentences
    new_df["token_found"] = word_found_list
    new_df["imgs"] = imgs_list
    new_df["picto"] = picto_list_str

    return new_df

df_total=download_data_cut("/content/output_SBU.txt", "/content/list_picto_SBU.txt", "/content/idx_picto_SBU.txt")

df_total.head(20)

df_total.drop_duplicates(subset=['sentence'], keep='first', inplace=True)
df_total = df_total[df_total['imgs'].apply(lambda x: len(x) > 0)]

df_total.head(20)

df_total.head(65956)

write_dataframe_to_text_file(df_total, 'SBU_preprocess.txt', "sentence")
write_dataframe_to_text_file(df_total, 'SBU_picto_preprc.txt', "picto")

!cp "/content/SBU_picto_preprc.txt" "/content/drive/MyDrive/VILT/Dataset_testo"
!cp "/content/SBU_preprocess.txt" "/content/drive/MyDrive/VILT/Dataset_testo"

write_dataframe_to_text_file(df_total, 'nopuctuation40_SBU_preprocess.txt', "sentence")
write_dataframe_to_text_file(df_total, 'nopuctuation40_SBU_picto_preprc.txt', "picto")
!cp "/content/nopuctuation40_SBU_preprocess.txt" "/content/drive/MyDrive/VILT/Dataset_testo"
!cp "/content/nopuctuation40_SBU_picto_preprc.txt" "/content/drive/MyDrive/VILT/Dataset_testo"

"""##Download dataset"""

sentence_word = open("/content/drive/MyDrive/VILT/Dataset_testo/SBU_preprocess.txt",'r').readlines()
sw = [s.rstrip() for s in sentence_word ]
len(sw)
df= pd.DataFrame(data=sw, columns=["sentence"])
df.size
#df["imgs"] = ""
#df["index"] = ""#index of the word sense into the dataframe pic_map
#df["token_found"] = ""
df.head()
piclist = open("/content/drive/MyDrive/VILT/Dataset_testo/SBU_picto_preprc.txt",'r').readlines()
pic = [s.rstrip() for s in piclist]
len(pic)
import pandas as pd
df_pic = pd.DataFrame(data=pic, columns=["picto"])
df_pic.head()

df_total=pd.concat([df, df_pic], axis=1)
df_total.head(100)

empty_rows = df_total[df_total["picto"].isnull()]
empty_rows_count = df_total["picto"].isnull().sum()
empty_rows_count

"""###STUDY of dataset"""

sentence_to_find = "green turtle catch by the daylight, they usually be not see after 4am more or less"
sentence_to_find="there be nothing about thailand .. we be here ; a beautiful place , a beautiful beach bar in vlichada , santorini"
result = df[df['sentence'] == sentence_to_find]
result

df_total= df_total[df_total['sentence'].apply(lambda x: len(x.split()) <= 40)]

# Calcola la lunghezza di ciascuna frase nella colonna 'sentence'
sentence_lengths = df_total['sentence'].apply(lambda x: len(x.split()))

# Calcola la lunghezza media delle frasi
max = sentence_lengths.max()

print("La frase di lunghezza massima è:")
print("Lunghezza:", max)
print(sentence_lengths)

sentence_to_find = "green turtle catch by the daylight, they usually be not see after 4am more or less"
sentence_to_find="there be nothing about thailand .. we be here ; a beautiful place , a beautiful beach bar in vlichada , santorini"
result = df[df['sentence'] == sentence_to_find]
result

import matplotlib.pyplot as plt

# Calcola la lunghezza di ciascuna frase nella colonna 'sentence'
sentence_lengths = df_total['sentence'].apply(lambda x: len(x.split()))

# Calcola la lunghezza media delle frasi
average_length = sentence_lengths.mean()
print("mean of length of sentence:", average_length )
# Crea un array di indici per le frasi
indices = range(len(sentence_lengths))

# Crea un grafico a barre per visualizzare le lunghezze delle frasi
plt.figure(figsize=(10, 6))
plt.bar(indices, sentence_lengths)
plt.axhline(y=average_length, color='red', linestyle='--', label='Average Length')
plt.xlabel('Sentence')
plt.ylabel('Length')
plt.title('Sentence Lengths')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Calcola il numero di token per ogni sentence nella colonna "picto"
df_total['token_count'] = df_total['picto'].apply(lambda x: len(x.split(' ')))

# Calcola il valore medio di token per sentence
average_token_count = np.mean(df_total['token_count'])
print("Mean number of tokens found into a sentence:", average_token_count)
# Crea un array di indici per le sentence
indices = np.arange(len(df_total))

# Imposta la larghezza delle barre
bar_width = 0.8

# Crea il grafico a barre
plt.figure(figsize=(10, 6))
plt.bar(indices, df_total['token_count'], width=bar_width)
plt.axhline(y=average_token_count, color='red', linestyle='--', label='Average Token Count')

# Personalizza l'asse x
plt.xlabel('Sentence')
plt.ylabel('Token Count')
plt.title('Token Count per Sentence')
plt.xticks(indices)

# Mostra la legenda
plt.legend()

# Visualizza il grafico
plt.show()

# Calcola il numero di picto per ogni sentence nella colonna "picto"
df_total['picto_count'] = df_total['picto'].apply(lambda x: len(x.split(' ')))

# Calcola il numero di frasi per ogni numero di picto
picto_counts = df_total['picto_count'].value_counts().sort_index()

# Crea un array di indici per il numero di picto
indices = np.arange(len(picto_counts))

# Imposta la larghezza delle barre
bar_width = 0.8

# Crea il grafico a barre
plt.figure(figsize=(10, 6))
plt.bar(indices, picto_counts, width=bar_width)
#plt.axhline(y=picto_counts.mean(), color='red', linestyle='--', label='Average Sentence Count')

# Personalizza l'asse x
plt.xlabel('Number of Picto per Sentence')
plt.ylabel('Sentence Count')
plt.title('Distribution of Picto per Sentence')
plt.xticks(indices, picto_counts.index)

# Mostra la legenda
#plt.legend()

# Visualizza il grafico
plt.show()

"""#Common Gen

CommonGen dataset is a dataset designed for automatic generation of natural language text (NLG). It was developed for the common text generation task, which consists of generating a sentence or short paragraph that consistently describes an object of given input.
we consider only the target col that is a string feature.

https://huggingface.co/datasets/common_gen

This code processes and analyzes the "CommonGen" dataset, including text preprocessing, data exploration, and saving preprocessed data to text files.

"""

def preprocess_punct(sentence):
    #convert to lowercase
    sentence = sentence.lower()
    # Remove URLs
    sentence = re.sub(r"http\S+|www\S+|https\S+", "", sentence, flags=re.MULTILINE)

    # Remove mentions
    sentence = re.sub(r"@\w+", "", sentence)

    # Replace genitive ('s) with " of "
    sentence = re.sub(r"'s\b", " of ", sentence)

    # Remove parentheses and their contents
    sentence = re.sub(r"\([^()]*\)", "", sentence)

    # Remove dashes and replace with space
    sentence = sentence.replace("-", " ")

    # Replace "no". with "number"
    sentence = sentence.replace("no.", "number")

    # Remove unrecognized characters
    sentence = re.sub(r"[^\w\s\d.,;!?]", "", sentence)

    # Remove punctuation
    sentence = re.sub(r"[.,;:?!]", "", sentence)

    # Remove multiple suspension points
    sentence = re.sub(r"\.{2,}", "", sentence)

    # Remove "⁇" symbol
    sentence = sentence.replace("⁇", "")

    # Tokenize sentence
    tokens = nltk.word_tokenize(sentence)

    # Initialize lemmatizer
    lemmatizer = WordNetLemmatizer()

    # Lemmatize verbs and keep punctuation
    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') if token.isalnum() else token for token in tokens]

    # Join tokens back into a sentence
    preprocessed_sentence = ' '.join(lemmatized_tokens)

    return preprocessed_sentence

def download_data_cut(file_path, picto_file_path, index_file_path):
    f = open(file_path, 'r')
    f2 = open(picto_file_path, 'w')
    f3 = open(index_file_path, 'w')
    new_sentences = []
    count = 0
    sentences = []
    word_found_list = []
    imgs_list = []
    picto_list_str = []
    new_df = pd.DataFrame(columns=["sentence", "token_found", "imgs", "picto"])

    for l in tqdm(f.readlines()):
        picto_list = []
        word_found = []  # Array to store matching tokens in each row
        processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
        tokenizer = processor.tokenizer
        #tokenized_sentence = tokenizer.tokenize(l.rstrip())
        tokenized_sentence = tokenizer.tokenize(l.rstrip(), truncation=True, max_length=40)


        # Check if the tokenized sentence exceeds the maximum length of 40 tokens
        if len(tokenized_sentence) > 40:
            continue

        # Check if at least one token is present in pic_map["word"] and has an associated image
        for token in tokenized_sentence:
            if (pic_map["word"].eq(token)).any():
                pictoid = pic_map.loc[pic_map["word"] == token, "pictogram_id"].tolist()[0]
                if os.path.exists("/content/pictoimg/Pittogrammi/" + str(int(pictoid)) + ".png"):
                    index = pic_map.loc[pic_map["word"] == token].index.tolist()
                    picto_list.append(int(pictoid))
                    word_found.append(token)

        # If no tokens satisfy the conditions, skip the current sentence
        # If picto_list is empty, it means that no token has satisfied the required conditions.
        if not picto_list:
            continue

        sentence = l.rstrip()
        sentences.append(sentence)
        word_found_list.append(' - '.join(word_found))
        imgs_list.append(picto_list)
        picto_list_str.append(' '.join(map(str, picto_list)))

    new_df["sentence"] = sentences
    new_df["token_found"] = word_found_list
    new_df["imgs"] = imgs_list
    new_df["picto"] = picto_list_str

    return new_df

from datasets import load_dataset
dataset = load_dataset("common_gen")
import pandas as pd
df = pd.DataFrame(dataset['train'])
df = df[['target']]

import pandas as pd
df = pd.DataFrame(dataset['train'])

df.head(20)

import pandas as pd

def write_dataframe_to_text_file(df, file_name, col):
    # Percorso completo del file di testo nella directory corrente
    file_path = f'./{file_name}'

    # Scrivi ogni riga del dataframe nel file di testo
    with open(file_path, 'w', encoding='utf-8') as file:
        for index, row in df.iterrows():
            line = row[col]
            #if isinstance(line, str) and line.strip():  # Verifica se la riga è una stringa non vuota
            file.write(str(line) + '\n')

    print(f"File '{file_name}' is created")

!pip install datasets transformers torch pytorch_lightning

from transformers import ViltConfig
from transformers import ViltProcessor, ViltForMaskedLM
import torch
processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
tokenizer = processor.tokenizer

df["preprocessed_sentence"] = df["target"].apply(preprocess_punct)

df.head()

df= df.head(20000) #10.000
write_dataframe_to_text_file( df, 'output_SBU.txt', "preprocessed_sentence")

df.head()

sentence_word = open("/content/output_SBU.txt",'r').readlines()
sw = [s.rstrip() for s in sentence_word ]
len(sw)
df= pd.DataFrame(data=sw, columns=["sentence"])
df.size

df.head()

df.size

import os
import random
import numpy as np
import torch
import pandas as pd
from urllib.request import urlopen
from zipfile import ZipFile
from io import BytesIO

from tqdm import tqdm

def download_data_cut(file_path, picto_file_path, index_file_path):
    f = open(file_path, 'r')
    f2 = open(picto_file_path, 'w')
    f3 = open(index_file_path, 'w')
    new_sentences = []
    count = 0
    sentences = []
    word_found_list = []
    imgs_list = []
    picto_list_str = []
    new_df = pd.DataFrame(columns=["sentence", "token_found", "imgs", "picto"])

    for l in tqdm(f.readlines()):
        picto_list = []
        word_found = []  # Array to store matching tokens in each row
        processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-mlm")
        tokenizer = processor.tokenizer
        #tokenized_sentence = tokenizer.tokenize(l.rstrip())
        tokenized_sentence = tokenizer.tokenize(l.rstrip(), truncation=True, max_length=40)


        # Check if the tokenized sentence exceeds the maximum length of 40 tokens
        if len(tokenized_sentence) > 40:
            continue

        # Check if at least one token is present in pic_map["word"] and has an associated image
        for token in tokenized_sentence:
            if (pic_map["word"].eq(token)).any():
                pictoid = pic_map.loc[pic_map["word"] == token, "pictogram_id"].tolist()[0]
                if os.path.exists("/content/pictoimg/Pittogrammi/" + str(int(pictoid)) + ".png"):
                    index = pic_map.loc[pic_map["word"] == token].index.tolist()
                    picto_list.append(int(pictoid))
                    word_found.append(token)

        # If no tokens satisfy the conditions, skip the current sentence
        # If picto_list is empty, it means that no token has satisfied the required conditions.
        if not picto_list:
            continue

        sentence = l.rstrip()
        sentences.append(sentence)
        word_found_list.append(' - '.join(word_found))
        imgs_list.append(picto_list)
        picto_list_str.append(' '.join(map(str, picto_list)))

    new_df["sentence"] = sentences
    new_df["token_found"] = word_found_list
    new_df["imgs"] = imgs_list
    new_df["picto"] = picto_list_str

    return new_df

df_total=download_data_cut("/content/output_SBU.txt", "/content/list_picto_CG.txt", "/content/idx_picto_CG.txt")

df_total.head(20)

df_total.drop_duplicates(subset=['sentence'], keep='first', inplace=True)
df_total = df_total[df_total['imgs'].apply(lambda x: len(x) > 0)]

df_total.head(20)

df_total.head(65956)

write_dataframe_to_text_file(df_total, 'nopuctuation40_CG_preprocess.tx', "sentence")
write_dataframe_to_text_file(df_total, 'nopuctuation40_CG_picto_preprc.txt', "picto")

!cp "/content/nopuctuation40_CG_preprocess.txt" "/content/drive/MyDrive/VILT/Dataset_testo"
!cp "/content/nopuctuation40_CG_picto_preprc.txt" "/content/drive/MyDrive/VILT/Dataset_testo"

"""##Download dataset"""

sentence_word = open("/content/drive/MyDrive/VILT/Dataset_testo/CG_preprocess.txt",'r').readlines()
sw = [s.rstrip() for s in sentence_word ]
len(sw)
df= pd.DataFrame(data=sw, columns=["sentence"])
df.size
#df["imgs"] = ""
#df["index"] = ""#index of the word sense into the dataframe pic_map
#df["token_found"] = ""
df.head()
piclist = open("/content/drive/MyDrive/VILT/Dataset_testo/CG_picto_preprc.txt",'r').readlines()
pic = [s.rstrip() for s in piclist]
len(pic)
import pandas as pd
df_pic = pd.DataFrame(data=pic, columns=["picto"])
df_pic.head()

df_total=pd.concat([df, df_pic], axis=1)
df_total.head(100)

empty_rows = df_total[df_total["picto"].isnull()]
empty_rows_count = df_total["picto"].isnull().sum()
empty_rows_count



"""###STUDY of dataset"""

# Calcola la lunghezza di ciascuna frase nella colonna 'sentence'
sentence_lengths = df_total['sentence'].apply(lambda x: len(x.split()))

# Calcola la lunghezza media delle frasi
max = sentence_lengths.max()

print("La frase di lunghezza massima è:")
print("Lunghezza:", max)
print(sentence_lengths)

import matplotlib.pyplot as plt

# Calcola la lunghezza di ciascuna frase nella colonna 'sentence'
sentence_lengths = df_total['sentence'].apply(lambda x: len(x.split()))

# Calcola la lunghezza media delle frasi
average_length = sentence_lengths.mean()
print("mean of length of sentence:", average_length )
# Crea un array di indici per le frasi
indices = range(len(sentence_lengths))

# Crea un grafico a barre per visualizzare le lunghezze delle frasi
plt.figure(figsize=(10, 6))
plt.bar(indices, sentence_lengths)
plt.axhline(y=average_length, color='red', linestyle='--', label='Average Length')
plt.xlabel('Sentence')
plt.ylabel('Length')
plt.title('Sentence Lengths')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Calcola il numero di token per ogni sentence nella colonna "picto"
df_total['token_count'] = df_total['picto'].apply(lambda x: len(x.split(' ')))

# Calcola il valore medio di token per sentence
average_token_count = np.mean(df_total['token_count'])
print("Mean number of tokens found into a sentence:", average_token_count)
# Crea un array di indici per le sentence
indices = np.arange(len(df_total))

# Imposta la larghezza delle barre
bar_width = 0.8

# Crea il grafico a barre
plt.figure(figsize=(10, 6))
plt.bar(indices, df_total['token_count'], width=bar_width)
plt.axhline(y=average_token_count, color='red', linestyle='--', label='Average Token Count')

# Personalizza l'asse x
plt.xlabel('Sentence')
plt.ylabel('Token Count')
plt.title('Token Count per Sentence')
plt.xticks(indices)

# Mostra la legenda
plt.legend()

# Visualizza il grafico
plt.show()

# Calcola il numero di picto per ogni sentence nella colonna "picto"
df_total['picto_count'] = df_total['picto'].apply(lambda x: len(x.split(' ')))

# Calcola il numero di frasi per ogni numero di picto
picto_counts = df_total['picto_count'].value_counts().sort_index()

# Crea un array di indici per il numero di picto
indices = np.arange(len(picto_counts))

# Imposta la larghezza delle barre
bar_width = 0.8

# Crea il grafico a barre
plt.figure(figsize=(10, 6))
plt.bar(indices, picto_counts, width=bar_width)
#plt.axhline(y=picto_counts.mean(), color='red', linestyle='--', label='Average Sentence Count')

# Personalizza l'asse x
plt.xlabel('Number of Picto per Sentence')
plt.ylabel('Sentence Count')
plt.title('Distribution of Picto per Sentence')
plt.xticks(indices, picto_counts.index)

# Mostra la legenda
#plt.legend()

# Visualizza il grafico
plt.show()

"""#KILOGRAM

https://huggingface.co/datasets/lil-lab/kilogram
"""

from datasets import load_dataset

dataset = load_dataset("lil-lab/kilogram")

import pandas as pd
df = pd.DataFrame(dataset)
#df = df[['target']]
df.head()